{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dd756-f7c5-402b-9c79-adcb0598f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from modelendpoints import query\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fd166-9692-42cc-9b34-21e434867f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_df = pd.read_parquet(\"INPUT_FILE\")\n",
    "decompose_df.head()\n",
    "print(decompose_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae8e3a-d754-4e00-a8b5-3c690b63789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_missing_prompt_llm_judge=\"\"\"\n",
    "You will be given a list of claims about a topic.\n",
    "\n",
    "You will be given an additional claim about the same topic. The new claim may be semantically similar to the claims in the list. \n",
    "\n",
    "Your task is to evaluate to whether this additional claim belongs in the list or not based on conveying some piece of information that is exactly in common with the majority of claims in the list. \n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "0: The New Claim may be related or very semantically similar to the claims in the Claim List, but doesnâ€™t clearly convey a piece of information EXACTLY in common with the majority of sentences in the Claim List \n",
    "1: The New Claim CLEARLY conveys some information EXACTLY in common with the MAJORITY of the sentences in the Claim List. Be strict about checking that the New Claim has information which is entailed by those sentences.\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the Claim List carefully.\n",
    "2. Read the New Claim carefully. \n",
    "3. Assign a score for the New Claim based on the Evaluation Criteria and the notes provided above. Please only respond with a single digit indicating the score.\n",
    "\n",
    "Example:\n",
    "\n",
    "Claim List: \n",
    "\n",
    "{claim_list}\n",
    "\n",
    "New Claim: \n",
    "\n",
    "{atomic_claim}\n",
    "\n",
    "Evaluation Form (score ONLY): -\"\"\"\n",
    "\n",
    "\n",
    "KEYS_TO_MESSAGES = {}\n",
    "KEYS_COUNTER = 1\n",
    "NUM_DECISIONS = 8\n",
    "ROLE = \"system\"\n",
    "\n",
    "l = decompose_df.shape[0]\n",
    "for i in range(l):\n",
    "    row = decompose_df.iloc[i]\n",
    "    claim_list = row[\"Cluster Text (For Missing)\"]\n",
    "    claim_list = \"\\n\".join(claim.strip() for claim in claim_list)\n",
    "    for claim in row[\"Missing Text\"]:\n",
    "        claim = claim.strip()\n",
    "        prompt_updated = cluster_missing_prompt_llm_judge.format(claim_list=claim_list,\n",
    "                                                                 atomic_claim=claim)\n",
    "        dict_row = [{\"role\": ROLE, \"content\": prompt_updated}]\n",
    "        KEYS_TO_MESSAGES[str(KEYS_COUNTER)]=dict_row\n",
    "        KEYS_COUNTER +=1)\n",
    "print(len(KEYS_TO_MESSAGES),len(KEYS_TO_MESSAGES)*NUM_DECISIONS)\n",
    "\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "print(OPENAI_KEY)\n",
    "client = openai.OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "decompose_output=query.openai_batch(client,keys_to_messages=KEYS_TO_MESSAGES,model=\"gpt-5\",\n",
    "                                reasoning_effort='minimal',\n",
    "                                temperature=1,\n",
    "                                top_p=1,\n",
    "                                frequency_penalty=0,\n",
    "                                presence_penalty=0,\n",
    "                                stop=None,\n",
    "                                n=NUM_DECISIONS\n",
    "                                )\n",
    "\n",
    "print(len(KEYS_TO_MESSAGES),len(decompose_output))\n",
    "for i in range(len(decompose_output)):\n",
    "    assert str(i+1) in decompose_output\n",
    "\n",
    "with open(\"OUTPUT_FILE\",\"w\") as f:\n",
    "    json.dump(decompose_output,f,indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a1685-3d25-49e4-91a8-a1ea4e32cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS_COUNTER = 1\n",
    "l = decompose_df.shape[0]\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = [-1]*l\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = decompose_df[\"GEVAL_GPT5_8\"].astype(object)\n",
    "ill_formatted = 0\n",
    "ill_formatted_rows = 0\n",
    "COUNT_ZERO=0\n",
    "COUNT_ONE=0\n",
    "\n",
    "for i in range(l):\n",
    "    row = decompose_df.iloc[i]\n",
    "    claim_rating_row = []\n",
    "    for _ in row[\"Missing Text\"]:\n",
    "        response=decompose_output.get(str(KEYS_COUNTER),{}).get('text',[])\n",
    "        claim_rating_llm=[]\n",
    "        for claim in response:\n",
    "            claim = claim.strip()\n",
    "            try:\n",
    "                claim_rating_llm.append(int(claim))\n",
    "            except Exception as e:\n",
    "                ill_formatted +=1\n",
    "        KEYS_COUNTER +=1\n",
    "        if claim_rating_llm:\n",
    "            cl = int(mode(claim_rating_llm).mode)\n",
    "            if cl:\n",
    "                COUNT_ONE+=1\n",
    "            else:\n",
    "                COUNT_ZERO+=1\n",
    "            claim_rating_row.append(cl)\n",
    "    if claim_rating_row:\n",
    "        decompose_df.at[i,\"GEVAL_GPT5_8\"]=claim_rating_row\n",
    "    else:\n",
    "        ill_formatted_rows+=1\n",
    "\n",
    "print(\"ILL FORMATTED OUTPUT\",ill_formatted)\n",
    "print(\"ILL FORMATTED ROWS\",ill_formatted_rows)\n",
    "print(\"0: \", COUNT_ZERO, \"1: \", COUNT_ONE)\n",
    "\n",
    "decompose_df.head()\n",
    "decompose_df.to_parquet(\"LLM_judge_OUTPUT_FILE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-modelendpoint",
   "language": "python",
   "name": "env-modelendpoint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
