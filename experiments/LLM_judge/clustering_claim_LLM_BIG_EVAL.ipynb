{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dd756-f7c5-402b-9c79-adcb0598f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from modelendpoints import query\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import scoreatpercentile,tmean,mode\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from scipy.stats import scoreatpercentile\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fd166-9692-42cc-9b34-21e434867f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_df = pd.read_parquet(\"INPUT_FILE\")\n",
    "decompose_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae8e3a-d754-4e00-a8b5-3c690b63789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_prompt_llm_judge=\"\"\"\n",
    "You will be given a short group of atomic claims (3-10 sentences) about a topic. \n",
    "\n",
    "Your task is to rate the group for the degree to which the sentences in the group form a cohesive cluster. Cohesion is defined in terms of each sentence in a Claim List conveying the same information\n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Cohesion (1-10) - cohesion is the degree to which the sentences in the Claim List for a cohesive set of claims. The most cohesive set of claims would be all sentences saying exactly the same thing. The least cohesive would be no sentences saying anything in common. Something in the middle (a 5) would be about half of the sentences conveying at least one thing in common.\n",
    "**Important:** Repetitions that are near exact match indicate very high cohesion.\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the Claim List carefully.\n",
    "2. Compare each sentence in the Claim List and try to determine how similar each sentence is in terms of the distinct, atomic information that they convey\n",
    "3. Assess the degree to which the Claim List is cohesive\n",
    "\n",
    "Example:\n",
    "\n",
    "Claim List: \n",
    "\n",
    "{claim_list}\n",
    "\n",
    "Evaluation Form (score ONLY): -\"\"\"\n",
    "\n",
    "KEYS_TO_MESSAGES = {}\n",
    "KEYS_COUNTER = 1\n",
    "NUM_DECISIONS = 8\n",
    "ROLE = \"system\"\n",
    "\n",
    "l = decompose_df.shape[0]\n",
    "for i in range(l):\n",
    "    row = decompose_df.iloc[i]\n",
    "    claim_list = list(row[\"Cluster Text (To Rate)\"])\n",
    "    prompt_updated = clustering_prompt_llm_judge.format(claim_list=claim_list)\n",
    "    dict_row = [{\"role\": ROLE, \"content\": prompt_updated}]\n",
    "    KEYS_TO_MESSAGES[str(KEYS_COUNTER)]=dict_row\n",
    "    KEYS_COUNTER +=1\n",
    "print(len(KEYS_TO_MESSAGES),len(KEYS_TO_MESSAGES)*NUM_DECISIONS)\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "print(OPENAI_KEY)\n",
    "client = openai.OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "decompose_output=query.openai_batch(client,keys_to_messages=KEYS_TO_MESSAGES,model=\"gpt-5\",\n",
    "                                reasoning_effort='minimal',\n",
    "                                temperature=1,\n",
    "                                top_p=1,\n",
    "                                frequency_penalty=0,\n",
    "                                presence_penalty=0,\n",
    "                                stop=None,\n",
    "                                n=NUM_DECISIONS\n",
    "                                )\n",
    "\n",
    "print(len(KEYS_TO_MESSAGES),len(decompose_output))\n",
    "for i in range(len(decompose_output)):\n",
    "    assert str(i+1) in decompose_output\n",
    "    \n",
    "with open(\"OUTPUT_FILE\",\"w\") as f:\n",
    "    json.dump(decompose_output,f,indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1fe78-1a2e-42ad-bbf7-d9535b30c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = decompose_df.shape[0]\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = [-1]*l\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = decompose_df[\"GEVAL_GPT5_8\"].astype(float)\n",
    "ill_formatted = 0\n",
    "ill_formatted_rows = 0\n",
    "\n",
    "for i in range(l):\n",
    "    cluster_rating_llm = decompose_output.get(str(i+1),{}).get('text',[])\n",
    "    cluster_rating_llm_new = []\n",
    "    for claim in cluster_rating_llm:\n",
    "        claim=claim.strip()\n",
    "        try:\n",
    "            if claim.isdigit():\n",
    "                cluster_rating_llm_new.append(max(1,float(claim)/2))\n",
    "            elif \":\" in claim: # to capture rating values like \"[Score: 2] or Score: 2\"\n",
    "                claim=claim.split(\":\")[-1]\n",
    "                claim = claim[:-1].strip() if \"]\" in claim else claim.strip()\n",
    "                cluster_rating_llm_new.append(max(1,float(claim)/2))\n",
    "            elif \"-\" in claim:\n",
    "                claim = claim.split(\"-\")[-1].strip() # to capture rating values like \"- 9\"\n",
    "                cluster_rating_llm_new.append(max(1,float(claim)/2))\n",
    "        except Exception as e:\n",
    "            ill_formatted +=1\n",
    "    if cluster_rating_llm_new:\n",
    "        cluster_rating_llm_new = float(tmean(cluster_rating_llm_new))\n",
    "        decompose_df.at[i,\"GEVAL_GPT5_8\"]=cluster_rating_llm_new\n",
    "    else:\n",
    "        print(i)\n",
    "        ill_formatted_rows+=1\n",
    "    \n",
    "print(\"ILL FORMATTED OUTPUT\",ill_formatted)\n",
    "print(\"ILL FORMATTED ROWS\",ill_formatted_rows)\n",
    "print(tmean(decompose_df[\"GEVAL_GPT5_8\"]),scoreatpercentile(decompose_df[\"GEVAL_GPT5_8\"],50),mode(decompose_df[\"GEVAL_GPT5_8\"]))\n",
    "\n",
    "decompose_df.to_parquet(\"LLM_judge_OUTPUT_FILE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-modelendpoint",
   "language": "python",
   "name": "env-modelendpoint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
