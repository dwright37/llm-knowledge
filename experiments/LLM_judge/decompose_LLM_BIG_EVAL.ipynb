{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dd756-f7c5-402b-9c79-adcb0598f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from modelendpoints import query\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import tmean\n",
    "from scipy.stats import scoreatpercentile, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fd166-9692-42cc-9b34-21e434867f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_df = pd.read_parquet(\"INPUT_FILE\")\n",
    "decompose_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae8e3a-d754-4e00-a8b5-3c690b63789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_prompt_llm_judge = \"\"\"\n",
    "You will be given a short paragraph (3-5 sentences) of text about the {topic} as a topic, and a single atomic claim obtained from that paragraph. \n",
    "Your task is to evaluate to what degree the claim is represented in the original short piece of text on a Likert scale.\n",
    "**Important**: The claim should be directly and explictly derived from the text without needed any extra external knowledge.\n",
    "\n",
    "**Important**: If the claim is partially but not fully represented in the paragraph, assign a score accordingly, rather than defaulting to extremes.\n",
    "\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "1 - The claim is totally irrelevant to the original paragraph OR does not explicitly talk about the provided TOPIC\n",
    "2 - The claim is incomplete or somewhat included in the original paragraph and missing clearly important context\n",
    "3 - The claim is included in the original paragraph but missing some potentially important context. This includes factoids which could be inferred from the original context but aren't explicitly stated (e.g., \"The lifecycle of plastic includes production.\" being inferred from \"It addresses the entire lifecycle of plastic, from production and consumption to disposal and recycling.\")\n",
    "4 - The claim is included in the original paragraph and is missing only unimportant context (the most important information is represented)\n",
    "5 - The claim is included in the original paragraph and no context is missing\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the paragraph carefully and the claim carefully. \n",
    "2. Assign a score for the claim based on the Evaluation Criteria and the Notes provided above. Please only respond with a single digit indicating the score.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Topic: \n",
    "\n",
    "{topic}\n",
    "\n",
    "Paragraph: \n",
    "\n",
    "{paragraph_chunk}\n",
    "\n",
    "Atomic claim: \n",
    "\n",
    "{atomic_claim}\n",
    "\n",
    "Evaluation Form (score ONLY): -\"\"\"\n",
    "\n",
    "\n",
    "KEYS_TO_MESSAGES = {}\n",
    "KEYS_COUNTER = 1\n",
    "NUM_DECISIONS = 8\n",
    "ROLE = \"system\"\n",
    "\n",
    "l = decompose_df.shape[0]\n",
    "for i in range(l):\n",
    "    row = decompose_df.iloc[i]\n",
    "    para = row[\"Chunk\"].strip()\n",
    "    topic = row[\"Topic\"].strip()\n",
    "    for claim in row[\"Claim List\"]:\n",
    "        claim = claim.strip()\n",
    "        prompt_updated = decompose_prompt_llm_judge.format(paragraph_chunk=para,atomic_claim=claim,topic=topic)\n",
    "        dict_row = [{\"role\": ROLE, \"content\": prompt_updated}]\n",
    "        KEYS_TO_MESSAGES[str(KEYS_COUNTER)]=dict_row\n",
    "        KEYS_COUNTER +=1\n",
    "        \n",
    "print(len(KEYS_TO_MESSAGES),len(KEYS_TO_MESSAGES)*NUM_DECISIONS)\n",
    "for i in range(len(KEYS_TO_MESSAGES)):\n",
    "    assert str(i+1) in KEYS_TO_MESSAGES\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "print(OPENAI_KEY)\n",
    "client = openai.OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "decompose_output=query.openai_batch(client,\n",
    "                                    keys_to_messages=KEYS_TO_MESSAGES,\n",
    "                                    model=\"gpt-5\",\n",
    "                                    reasoning_effort='minimal',\n",
    "                                    temperature=1,\n",
    "                                    top_p=1,\n",
    "                                    frequency_penalty=0,\n",
    "                                    presence_penalty=0,\n",
    "                                    stop=None,\n",
    "                                    n=NUM_DECISIONS\n",
    "                                )\n",
    "\n",
    "print(len(KEYS_TO_MESSAGES),len(decompose_output))\n",
    "for i in range(len(decompose_output)):\n",
    "    assert str(i+1) in decompose_output\n",
    "\n",
    "with open(\"OUTPUT_FILE\",\"w\") as f:\n",
    "    json.dump(decompose_output,f,indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b18cb-1adb-4a21-9820-26890473ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = decompose_df.shape[0]\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = [-1]*l\n",
    "## so that we can later assign a list, else dtype remains int but assignment becomes an object...gives error \n",
    "decompose_df[\"GEVAL_GPT5_8\"] = decompose_df[\"GEVAL_GPT5_8\"].astype(object) \n",
    "ill_formatted = 0\n",
    "ill_formatted_rows = 0\n",
    "OUTPUT_COUNTER = 1\n",
    "for i in range(l):\n",
    "    row = decompose_df.iloc[i]\n",
    "    claim_list_human = row[\"Claim List\"]\n",
    "    ch = len(claim_list_human)\n",
    "    claim_rating_list = []\n",
    "    claim_rating_list_final=[]\n",
    "    for _ in range(ch):\n",
    "        response = decompose_output.get(str(OUTPUT_COUNTER),{}).get('text',[])\n",
    "        claim_rating_list.append(response)            \n",
    "        OUTPUT_COUNTER+=1\n",
    "\n",
    "    for claim_l in claim_rating_list:\n",
    "        claim_rating_llm_new = []\n",
    "        for o in claim_l:\n",
    "            try:\n",
    "                claim_rating_llm_new.append(float(o))\n",
    "            except Exception as e:\n",
    "                ill_formatted +=1\n",
    "        if claim_rating_llm_new:\n",
    "            claim_rating_llm_new = float(tmean(claim_rating_llm_new)) ## output is a np.datatype hence converting to float for sanity.\n",
    "        else:\n",
    "            claim_rating_llm_new = -1\n",
    "        claim_rating_list_final.append(claim_rating_llm_new)\n",
    "    if claim_rating_list_final:\n",
    "        decompose_df.at[i,\"GEVAL_GPT5_8\"]=claim_rating_list_final\n",
    "    else:\n",
    "        ill_formatted_rows+=1\n",
    "print(\"ILL FORMATTED OUTPUT\",ill_formatted)\n",
    "print(\"ILL FORMATTED ROWS\",ill_formatted_rows)\n",
    "\n",
    "decompose_df.to_parquet(\"LLM_judge_OUTPUT_FILE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-modelendpoint",
   "language": "python",
   "name": "env-modelendpoint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
