{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dd756-f7c5-402b-9c79-adcb0598f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from modelendpoints import query\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import tmean\n",
    "from scipy.stats import scoreatpercentile, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fd166-9692-42cc-9b34-21e434867f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_df = pd.read_parquet(\"INPUT_FILE\")\n",
    "decompose_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae8e3a-d754-4e00-a8b5-3c690b63789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_prompt_llm_judge=\"\"\"\n",
    "You will be given a short paragraph (3-5 sentences) of text about a topic, and a list of atomic claims extracted from it. \n",
    "Your task is to report the number of atomic claims present in the paragraph that are missing from the list. \n",
    "\n",
    "*Important:* You DO NOT have to return the text  of missing claims, JUST REPORT THE MISSING COUNT.\n",
    "Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "0: Every single claim that could possibly be inferred from the original paragraph is present in the Claim List\n",
    "1: At least 1 claim is missing from the Claim List\n",
    "2: At least 2 claims are missing from the Claim List\n",
    "3: At least 3 claims are missing from the Claim List\n",
    "4: At least 4 claims are missing from the Claim List\n",
    "5+: The same as above, for any number greater than or equal to 5\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the paragraph and the claim list carefully.\n",
    "2. Compare the provided list of atomic claims to the original paragraph.\n",
    "3. Count all the atomic claims in the paragraph that are not included in the list. \n",
    "\n",
    "Example:\n",
    "Topic: \n",
    "\n",
    "{topic}\n",
    "\n",
    "Paragraph: \n",
    "\n",
    "{paragraph_chunk}\n",
    "\n",
    "Claim List: \n",
    "\n",
    "{claim_list}\n",
    "\n",
    "\n",
    "Evaluation Form (score ONLY): -\"\"\"\n",
    "\n",
    "KEYS_TO_MESSAGES = {}\n",
    "KEYS_COUNTER = 1\n",
    "NUM_DECISIONS = 8 ## for gpt-5 this is the max allowed.\n",
    "ROLE = \"system\"\n",
    "\n",
    "l = decompose_df.shape[0]\n",
    "for i in range(l):\n",
    "    row = decompose_df.iloc[i]\n",
    "    para = row[\"Chunk\"].strip()\n",
    "    claim_list = row[\"Claim List\"]\n",
    "    topic = row[\"Topic\"].strip()\n",
    "    claim_list = \"\\n\".join(claim.strip() for claim in claim_list)\n",
    "    prompt_updated = missing_prompt_llm_judge.format(paragraph_chunk=para,claim_list=claim_list,topic=topic)\n",
    "    dict_row = [{\"role\": ROLE, \"content\": prompt_updated}]\n",
    "    KEYS_TO_MESSAGES[str(KEYS_COUNTER)]=dict_row\n",
    "    KEYS_COUNTER +=1\n",
    "    \n",
    "print(len(KEYS_TO_MESSAGES),len(KEYS_TO_MESSAGES)*NUM_DECISIONS)\n",
    "print(KEYS_TO_MESSAGES[\"1\"][0][\"content\"])\n",
    "\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "print(OPENAI_KEY)\n",
    "client = openai.OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "decompose_output=query.openai_batch(client,keys_to_messages=KEYS_TO_MESSAGES,model=\"gpt-5\",\n",
    "                                reasoning_effort='minimal',\n",
    "                                temperature=1,\n",
    "                                top_p=1,\n",
    "                                frequency_penalty=0,\n",
    "                                presence_penalty=0,\n",
    "                                stop=None,\n",
    "                                n=NUM_DECISIONS\n",
    "                                )\n",
    "\n",
    "print(len(KEYS_TO_MESSAGES),len(decompose_output))\n",
    "for i in range(len(decompose_output)):\n",
    "    assert str(i+1) in decompose_output\n",
    "\n",
    "with open(\"OUTPUT_FILE\",\"w\") as f:\n",
    "    json.dump(decompose_output,f,indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1fe78-1a2e-42ad-bbf7-d9535b30c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = decompose_df.shape[0]\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = [-1]*l\n",
    "decompose_df[\"GEVAL_GPT5_8\"] = decompose_df[\"GEVAL_GPT5_8\"].astype(float) \n",
    "ill_formatted = 0\n",
    "ill_formatted_rows = 0\n",
    "for i in range(l):\n",
    "    claim_rating_llm = decompose_output.get(str(i+1),{}).get('text',[])\n",
    "    claim_rating_llm_new = []\n",
    "    for claim in claim_rating_llm:\n",
    "        try:\n",
    "            if claim.isdigit():\n",
    "                claim_rating_llm_new.append(float(claim))\n",
    "            else:\n",
    "                claim_rating_llm_new.append(float(claim[:-1])) ## to capture rating values like \"5+\", \"3+\" \n",
    "        except Exception as e:\n",
    "            # print(claim)\n",
    "            ill_formatted +=1\n",
    "    if claim_rating_llm_new:\n",
    "        claim_rating_llm_new = float(tmean(claim_rating_llm_new)) ## output is a np.datatype hence converting to float for sanity.\n",
    "        decompose_df.at[i,\"GEVAL_GPT5_8\"]=claim_rating_llm_new\n",
    "    else:\n",
    "        print(i)\n",
    "        ill_formatted_rows+=1\n",
    "        \n",
    "print(\"ILL FORMATTED OUTPUT\",ill_formatted)\n",
    "print(\"ILL FORMATTED ROWS\",ill_formatted_rows)\n",
    "\n",
    "print(tmean(decompose_df[\"GEVAL_GPT5_8\"]),scoreatpercentile(decompose_df[\"GEVAL_GPT5_8\"],50),mode(decompose_df[\"GEVAL_GPT5_8\"]))\n",
    "decompose_df.to_parquet(\"LLM_judge_OUTPUT_FILE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-modelendpoint",
   "language": "python",
   "name": "env-modelendpoint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
